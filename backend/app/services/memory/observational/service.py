"""Core Observational Memory service: Observer and Reflector logic."""

from __future__ import annotations

import json
import logging
from dataclasses import dataclass
from datetime import datetime, timezone
from typing import Any

import httpx

from app.db.models.observation import Observation
from app.services.memory.observational.prompts import (
    OBSERVATION_CONTINUATION_HINT,
    build_observer_prompt,
    build_reflector_prompt,
    parse_observation_output,
)
from app.services.token_counter import count_messages_tokens
from app.utils.ids import generate_id
from app.utils.messages import strip_message_metadata
from app.utils.time import utc_now_iso

logger = logging.getLogger(__name__)

_OBSERVER_MAX_TOKENS = 8192
_REFLECTOR_MAX_TOKENS = 8192


class ObservationError(RuntimeError):
    """Raised when observer/reflector requests fail and should be surfaced to UI."""


@dataclass
class BufferedObservationChunk:
    """Dormant observation chunk generated by async buffering."""

    content: str
    token_count: int
    observed_up_to_message_id: str | None
    observed_up_to_timestamp: str | None
    current_task: str | None = None
    suggested_response: str | None = None
    trigger_token_count: int | None = None

    def to_dict(self) -> dict[str, Any]:
        return {
            "content": self.content,
            "tokenCount": self.token_count,
            "triggerTokenCount": self.trigger_token_count,
            "observedUpToMessageId": self.observed_up_to_message_id,
            "observedUpToTimestamp": self.observed_up_to_timestamp,
            "currentTask": self.current_task,
            "suggestedResponse": self.suggested_response,
        }

    @classmethod
    def from_dict(cls, raw: dict[str, Any]) -> "BufferedObservationChunk | None":
        content = raw.get("content")
        if not isinstance(content, str) or not content.strip():
            return None
        token_count = raw.get("tokenCount")
        trigger_token_count = raw.get("triggerTokenCount")
        return cls(
            content=content.strip(),
            token_count=int(token_count) if isinstance(token_count, int) else _count_tokens(content),
            trigger_token_count=int(trigger_token_count) if isinstance(trigger_token_count, int) and trigger_token_count > 0 else None,
            observed_up_to_message_id=(
                str(raw.get("observedUpToMessageId"))
                if raw.get("observedUpToMessageId") is not None
                else None
            ),
            observed_up_to_timestamp=(
                str(raw.get("observedUpToTimestamp"))
                if raw.get("observedUpToTimestamp") is not None
                else None
            ),
            current_task=(
                str(raw.get("currentTask")).strip()
                if isinstance(raw.get("currentTask"), str) and str(raw.get("currentTask")).strip()
                else None
            ),
            suggested_response=(
                str(raw.get("suggestedResponse")).strip()
                if isinstance(raw.get("suggestedResponse"), str) and str(raw.get("suggestedResponse")).strip()
                else None
            ),
        )


def _count_tokens(text: str) -> int:
    """Count tokens using tiktoken (cl100k_base), falling back to char/4 estimate."""
    if not text:
        return 0
    try:
        import tiktoken
        enc = tiktoken.get_encoding("cl100k_base")
        return len(enc.encode(text)) or 1
    except Exception:
        return max(1, len(text) // 4)


def _today_str() -> str:
    """Return today's date as a human-readable string like 'February 19, 2026'."""
    return datetime.now(timezone.utc).strftime("%B %d, %Y")


def _format_observation_error(exc: Exception) -> str:
    """Create a short user-safe error string from provider/client exceptions."""
    if isinstance(exc, httpx.HTTPStatusError):
        status = exc.response.status_code
        detail = ""
        try:
            body = exc.response.text.strip()
            if body:
                detail = body[:240]
        except Exception:
            detail = ""
        return f"Observer request failed ({status}). {detail}".strip()
    return str(exc) or "Observer request failed."


def _as_timestamp(value: Any) -> str | None:
    if isinstance(value, str):
        out = value.strip()
        return out or None
    return None


def _parse_iso_timestamp(value: Any) -> datetime | None:
    raw = _as_timestamp(value)
    if raw is None:
        return None
    normalized = raw[:-1] + "+00:00" if raw.endswith("Z") else raw
    try:
        parsed = datetime.fromisoformat(normalized)
    except ValueError:
        return None
    if parsed.tzinfo is None:
        return parsed.replace(tzinfo=timezone.utc)
    return parsed


def _timestamp_is_on_or_before(left: Any, right: Any) -> bool:
    left_raw = _as_timestamp(left)
    right_raw = _as_timestamp(right)
    if left_raw is None or right_raw is None:
        return False

    left_ts = _parse_iso_timestamp(left_raw)
    right_ts = _parse_iso_timestamp(right_raw)
    if left_ts is not None and right_ts is not None:
        return left_ts <= right_ts

    return left_raw <= right_raw


def _latest_timestamp(left: str | None, right: str | None) -> str | None:
    if left is None:
        return right
    if right is None:
        return left
    left_ts = _parse_iso_timestamp(left)
    right_ts = _parse_iso_timestamp(right)
    if left_ts is not None and right_ts is not None:
        return right if right_ts > left_ts else left
    return right if right > left else left


class ObservationMemoryService:
    """Runs Observer and Reflector to maintain observation log for a chat."""

    def __init__(self, chat_repo) -> None:
        self._chat_repo = chat_repo

    def get_observational_state(
        self,
        chat_id: str,
        *,
        default_buffer_tokens: int,
    ) -> dict[str, Any]:
        """Read and normalize the observational memory state JSON."""
        state_row = self._chat_repo.get_memory_state(chat_id)
        parsed: dict[str, Any] = {}
        if (
            state_row is not None
            and state_row.strategy == "observational"
            and isinstance(state_row.state_json, str)
        ):
            try:
                raw = json.loads(state_row.state_json)
                if isinstance(raw, dict):
                    parsed = raw
            except Exception:
                parsed = {}

        buffer_candidate = parsed.get("buffer")
        if isinstance(buffer_candidate, dict):
            buffer_raw: dict[str, Any] = buffer_candidate
        else:
            buffer_raw = {}

        normalized_chunks: list[dict[str, Any]] = []
        buffer_chunks = buffer_raw.get("chunks")
        if isinstance(buffer_chunks, list):
            for item in buffer_chunks:
                if not isinstance(item, dict):
                    continue
                chunk = BufferedObservationChunk.from_dict(item)
                if chunk is not None:
                    normalized_chunks.append(chunk.to_dict())

        buffer_tokens_candidate = buffer_raw.get("tokens")
        buffer_tokens: int = (
            buffer_tokens_candidate
            if isinstance(buffer_tokens_candidate, int) and buffer_tokens_candidate > 0
            else max(500, default_buffer_tokens)
        )
        buffer_last_boundary_candidate = buffer_raw.get("lastBoundary")
        buffer_last_boundary: int = (
            buffer_last_boundary_candidate
            if isinstance(buffer_last_boundary_candidate, int) and buffer_last_boundary_candidate >= 0
            else 0
        )
        buffer_up_to_message_id_candidate = buffer_raw.get("upToMessageId")
        buffer_up_to_message_id = (
            str(buffer_up_to_message_id_candidate)
            if buffer_up_to_message_id_candidate is not None
            else None
        )
        buffer_up_to_timestamp_candidate = buffer_raw.get("upToTimestamp")
        buffer_up_to_timestamp = (
            str(buffer_up_to_timestamp_candidate)
            if buffer_up_to_timestamp_candidate is not None
            else None
        )

        return {
            "generation": parsed.get("generation"),
            "tokenCount": parsed.get("tokenCount"),
            "triggerTokenCount": parsed.get("triggerTokenCount"),
            "observedUpToMessageId": parsed.get("observedUpToMessageId"),
            "currentTask": parsed.get("currentTask"),
            "suggestedResponse": parsed.get("suggestedResponse"),
            "timestamp": parsed.get("timestamp"),
            "buffer": {
                "tokens": buffer_tokens,
                "lastBoundary": buffer_last_boundary,
                "upToMessageId": buffer_up_to_message_id,
                "upToTimestamp": buffer_up_to_timestamp,
                "chunks": normalized_chunks,
            },
        }

    def save_observational_state(self, chat_id: str, state: dict[str, Any]) -> None:
        """Persist observational memory state JSON."""
        self._chat_repo.set_memory_state(
            chat_id=chat_id,
            strategy="observational",
            state_json=json.dumps(state),
            updated_at=utc_now_iso(),
        )
        self._chat_repo.commit()

    def update_state_from_observation(
        self,
        *,
        state: dict[str, Any],
        observation: Observation | None,
    ) -> dict[str, Any]:
        """Mirror latest active observation fields into state JSON."""
        if observation is None:
            return state
        out = dict(state)
        out["generation"] = observation.generation
        out["tokenCount"] = observation.token_count
        out["triggerTokenCount"] = (
            observation.trigger_token_count
            if isinstance(observation.trigger_token_count, int)
            and observation.trigger_token_count > 0
            else observation.token_count
        )
        out["observedUpToMessageId"] = observation.observed_up_to_message_id
        out["currentTask"] = observation.current_task
        out["suggestedResponse"] = observation.suggested_response
        out["timestamp"] = observation.timestamp
        return out

    def split_messages_by_waterline(
        self,
        all_messages: list[dict],
        *,
        waterline_message_id: str | None,
        waterline_timestamp: str | None,
    ) -> tuple[list[dict], list[dict]]:
        """
        Split messages into (observed, unobserved) with dual waterline support.

        - `_message_id` rows are tracked by message-id waterline.
        - synthetic rows without `_message_id` are tracked by timestamp waterline.
        """
        if not all_messages:
            return [], []

        observed_flags = [False] * len(all_messages)
        message_anchor_found = False

        if waterline_message_id:
            waterline_idx: int | None = None
            for i, msg in enumerate(all_messages):
                if msg.get("_message_id") == waterline_message_id:
                    waterline_idx = i
                    break
            if waterline_idx is not None:
                message_anchor_found = True
                for i in range(0, waterline_idx + 1):
                    if all_messages[i].get("_message_id") is not None:
                        observed_flags[i] = True

        if waterline_timestamp:
            for i, msg in enumerate(all_messages):
                # Primary tracking for real messages is message-id waterline. If that
                # anchor is absent or no longer present, fall back to timestamp so
                # pre-observed history is not re-counted as newly unobserved.
                if msg.get("_message_id") is not None and message_anchor_found:
                    continue
                ts = msg.get("_timestamp") or msg.get("timestamp")
                if _timestamp_is_on_or_before(ts, waterline_timestamp):
                    observed_flags[i] = True

        observed: list[dict] = []
        unobserved: list[dict] = []
        for msg, is_observed in zip(all_messages, observed_flags, strict=False):
            if is_observed:
                observed.append(msg)
            else:
                unobserved.append(msg)
        return observed, unobserved

    def get_unobserved_messages(
        self,
        all_messages: list[dict],
        observation: Observation | None,
    ) -> tuple[list[dict], list[dict]]:
        """Split messages into (observed, unobserved) based on active observation waterline."""
        return self.split_messages_by_waterline(
            all_messages,
            waterline_message_id=(
                observation.observed_up_to_message_id if observation is not None else None
            ),
            waterline_timestamp=(observation.timestamp if observation is not None else None),
        )

    def _waterline_from_messages(
        self,
        messages: list[dict],
    ) -> tuple[str | None, str | None]:
        """Find latest message-id and timestamp in a message slice."""
        last_msg_id: str | None = None
        last_timestamp: str | None = None
        for msg in reversed(messages):
            if last_timestamp is None:
                last_timestamp = _as_timestamp(msg.get("_timestamp") or msg.get("timestamp"))
            if last_msg_id is None:
                mid = msg.get("_message_id")
                if isinstance(mid, str) and mid:
                    last_msg_id = mid
            if last_msg_id is not None and last_timestamp is not None:
                break
        return last_msg_id, last_timestamp

    async def run_observer_for_chunk(
        self,
        *,
        messages: list[dict],
        model: str,
        observer_model: str | None,
        client,
        trigger_token_count: int | None = None,
        prior_chunks: list[str] | None = None,
    ) -> BufferedObservationChunk | None:
        """Run observer on a specific unobserved slice and return a dormant chunk."""
        if not messages:
            return None

        clean_messages = strip_message_metadata(messages)
        prompt_messages = build_observer_prompt(
            existing_observation=None,
            new_messages=clean_messages,
            today=_today_str(),
            prior_chunks=prior_chunks,
        )
        effective_model = observer_model or model

        try:
            raw_output = await client.create_chat_completion(
                model=effective_model,
                messages=prompt_messages,
                max_tokens=_OBSERVER_MAX_TOKENS,
                temperature=0.1,
            )
        except Exception as exc:
            logger.warning("Observer LLM call failed", exc_info=True)
            raise ObservationError(_format_observation_error(exc)) from exc

        content, current_task, suggested_response = parse_observation_output(raw_output)
        if not content:
            return None

        last_msg_id, last_timestamp = self._waterline_from_messages(messages)
        if last_timestamp is None:
            last_timestamp = utc_now_iso()

        return BufferedObservationChunk(
            content=content,
            token_count=_count_tokens(content),
            trigger_token_count=trigger_token_count if isinstance(trigger_token_count, int) and trigger_token_count > 0 else None,
            observed_up_to_message_id=last_msg_id,
            observed_up_to_timestamp=last_timestamp,
            current_task=current_task,
            suggested_response=suggested_response,
        )

    async def run_observer(
        self,
        *,
        chat_id: str,
        messages: list[dict],
        model: str,
        observer_model: str | None,
        client,
    ) -> Observation | None:
        """
        Legacy observer path: summarize all currently unobserved messages into active state.

        Background buffering now uses `run_observer_for_chunk()` and activation.
        """
        latest_obs = self._chat_repo.get_latest_observation(chat_id)
        _observed, unobserved = self.get_unobserved_messages(messages, latest_obs)
        unobserved_token_count = count_messages_tokens(unobserved, model=model)
        chunk = await self.run_observer_for_chunk(
            messages=unobserved,
            model=model,
            observer_model=observer_model,
            client=client,
            trigger_token_count=unobserved_token_count,
        )
        if chunk is None:
            return None
        return self.activate_buffered_observations(
            chat_id=chat_id,
            base_observation=latest_obs,
            chunks=[chunk],
            trigger_token_count=unobserved_token_count,
        )

    def activate_buffered_observations(
        self,
        *,
        chat_id: str,
        base_observation: Observation | None,
        chunks: list[BufferedObservationChunk],
        trigger_token_count: int | None = None,
    ) -> Observation | None:
        """Concatenate buffered chunks into the active observation block."""
        if not chunks:
            return base_observation

        content_parts: list[str] = []
        if base_observation and base_observation.content.strip():
            content_parts.append(base_observation.content.strip())
        for chunk in chunks:
            if chunk.content.strip():
                content_parts.append(chunk.content.strip())
        merged_content = "\n\n".join(content_parts).strip()
        if not merged_content:
            return base_observation

        # Each activation creates a new observation generation:
        # first activation starts at 0, then increments on every subsequent activation.
        # If no base_observation was passed, check the DB for the latest generation so
        # that the counter always increments even when the caller doesn't hold a reference.
        effective_base = base_observation
        if effective_base is None:
            effective_base = self._chat_repo.get_latest_observation(chat_id)
        generation = (effective_base.generation + 1) if effective_base is not None else 0
        observed_up_to_message_id = (
            base_observation.observed_up_to_message_id if base_observation is not None else None
        )
        observed_up_to_timestamp = (
            base_observation.timestamp if base_observation is not None else None
        )
        current_task = base_observation.current_task if base_observation is not None else None
        suggested_response = (
            base_observation.suggested_response if base_observation is not None else None
        )

        for chunk in chunks:
            if chunk.observed_up_to_message_id:
                observed_up_to_message_id = chunk.observed_up_to_message_id
            observed_up_to_timestamp = _latest_timestamp(
                observed_up_to_timestamp,
                chunk.observed_up_to_timestamp,
            )
            if chunk.current_task:
                current_task = chunk.current_task
            if chunk.suggested_response:
                suggested_response = chunk.suggested_response

        obs = self._chat_repo.create_observation(
            observation_id=generate_id("obs"),
            chat_id=chat_id,
            generation=generation,
            content=merged_content,
            token_count=_count_tokens(merged_content),
            trigger_token_count=(
                trigger_token_count
                if isinstance(trigger_token_count, int) and trigger_token_count > 0
                else None
            ),
            observed_up_to_message_id=observed_up_to_message_id,
            current_task=current_task,
            suggested_response=suggested_response,
            timestamp=observed_up_to_timestamp or utc_now_iso(),
        )
        self._chat_repo.commit()
        logger.info(
            "Activated %d buffered chunks for chat=%s gen=%d tokens=%d",
            len(chunks),
            chat_id,
            generation,
            obs.token_count,
        )
        return obs

    async def run_reflector(
        self,
        *,
        chat_id: str,
        model: str,
        reflector_model: str | None,
        reflector_threshold: int,
        client,
    ) -> Observation | None:
        """
        Run the Reflector if the current observation is too large.

        Returns a new higher-generation Observation if reflector ran, else None.
        """
        latest_obs = self._chat_repo.get_latest_observation(chat_id)
        if latest_obs is None:
            return None

        if latest_obs.token_count < reflector_threshold:
            return None

        effective_model = reflector_model or model
        prompt_messages = build_reflector_prompt(latest_obs.content)

        try:
            raw_output = await client.create_chat_completion(
                model=effective_model,
                messages=prompt_messages,
                max_tokens=_REFLECTOR_MAX_TOKENS,
                temperature=0.1,
            )
        except Exception as exc:
            logger.warning(
                "Reflector LLM call failed for chat=%s", chat_id, exc_info=True
            )
            raise ObservationError(_format_observation_error(exc)) from exc

        content, current_task, suggested_response = parse_observation_output(raw_output)
        if not content:
            return None

        new_generation = latest_obs.generation + 1
        token_count = _count_tokens(content)

        new_obs = self._chat_repo.create_observation(
            observation_id=generate_id("obs"),
            chat_id=chat_id,
            generation=new_generation,
            content=content,
            token_count=token_count,
            trigger_token_count=latest_obs.token_count,
            observed_up_to_message_id=latest_obs.observed_up_to_message_id,
            current_task=current_task,
            suggested_response=suggested_response,
            timestamp=latest_obs.timestamp,
        )
        # Delete superseded observations
        self._chat_repo.delete_observations_before_generation(chat_id, new_generation)
        self._chat_repo.commit()

        logger.info(
            "Reflector created observation for chat=%s gen=%d tokens=%d (was %d)",
            chat_id,
            new_generation,
            token_count,
            latest_obs.token_count,
        )
        return new_obs

    def build_context_with_observations(
        self,
        *,
        observation: Observation,
        unobserved_messages: list[dict],
        system_prompt_msg: dict | None,
    ) -> list[dict]:
        """
        Assemble context: [system] [observations block] [continuation hint] [recent msgs].

        The system_prompt_msg is the existing system message from the pipeline (already prepended).
        """
        result: list[dict] = []

        if system_prompt_msg:
            result.append(system_prompt_msg)

        obs_block = self.format_observations_for_context(
            observations=observation.content,
            current_task=observation.current_task,
        )
        result.append({"role": "system", "content": obs_block})

        continuation_hint = OBSERVATION_CONTINUATION_HINT
        if observation.suggested_response:
            continuation_hint = (
                continuation_hint
                + "\n\nSuggested continuation from memory: "
                + observation.suggested_response
            )
        result.append({"role": "system", "content": continuation_hint})

        # Append unobserved messages (stripped of internal metadata)
        clean_unobserved = strip_message_metadata(unobserved_messages)
        result.extend(clean_unobserved)

        return result

    def format_observations_for_context(
        self,
        observations: str,
        current_task: str | None,
    ) -> str:
        """Format observations as a system message for context injection."""
        lines = [
            "<observations>",
            "The following is a structured memory of this conversation so far.",
            "Treat it as your long-term memory. Prefer the MOST RECENT information for",
            "any conflicting facts. Assume planned actions in the past were completed",
            "unless explicitly stated otherwise. Do not mention this memory system â€” just",
            "use the information naturally.",
            "",
            observations,
        ]

        if current_task:
            lines.append("")
            lines.append(f"**Current task**: {current_task}")

        lines.append("</observations>")
        return "\n".join(lines)
